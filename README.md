# MLOps Project - Vehicle Insurance Data Pipeline

Welcome to this MLOps project, designed to demonstrate a robust pipeline for managing vehicle insurance data. This project aims to impress recruiters and visitors by showcasing the various tools, techniques, services, and features that go into building and deploying a machine learning pipeline for real-world data management. Follow along to learn about project setup, data processing, model deployment, and CI/CD automation!

---

## ğŸ“ Project Setup and Structure

### ğŸ› ï¸ Step 1: Project Template
Start by executing the `template.py` file to create the initial project template, which includes the required folder structure and placeholder files.

### ğŸ“¦ Step 2: Package Management
Write the setup for importing local packages in `setup.py` and `pyproject.toml` files.  
**Tip**: Learn more about these files from `crashcourse.txt`.

### ğŸŒ Step 3: Virtual Environment and Dependencies
Create a virtual environment and install required dependencies from `requirements.txt`:
```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
```
Verify the local packages by running:
```bash
pip list
```

---

## ğŸ“Š MongoDB Setup and Data Management

### ğŸ—„ï¸ Step 4: MongoDB Atlas Configuration
- Sign up for MongoDB Atlas and create a new project.
- Set up a free M0 cluster, configure the username and password, and allow access from any IP address (`0.0.0.0/0`).
- Retrieve the MongoDB connection string for Python and save it (replace `<password>` with your password).

### ğŸ“‚ Step 5: Pushing Data to MongoDB
- Create a folder named `notebook`, add the dataset, and create a notebook file `mongoDB_demo.ipynb`.
- Use the notebook to push data to the MongoDB database.
- Verify the data in MongoDB Atlas under **Database > Browse Collections**.

---

## ğŸ“ Logging, Exception Handling, and EDA

### ğŸ” Step 6: Set Up Logging and Exception Handling
- Create logging and exception handling modules.
- Test them on a demo file `demo.py`.

### ğŸ“ˆ Step 7: Exploratory Data Analysis (EDA) and Feature Engineering
- Analyze and engineer features in the **EDA and Feature Engg** notebook for further processing in the pipeline.

---

## ğŸ“¥ Data Ingestion

### ğŸ› ï¸ Step 8: Data Ingestion Pipeline
- Define MongoDB connection functions in `configuration.mongo_db_connections.py`.
- Develop data ingestion components in the `data_access` and `components.data_ingestion.py` files to fetch and transform data.
- Update `entity/config_entity.py` and `entity/artifact_entity.py` with relevant ingestion configurations.
- Run `demo.py` after setting up the MongoDB connection as an environment variable.

#### âš™ï¸ Setting Environment Variables
Set MongoDB URL:
```bash
# For Bash
export MONGODB_URL="mongodb+srv://<username>:<password>...."

# For Powershell
$env:MONGODB_URL = "mongodb+srv://<username>:<password>...."
```
**Note**: On Windows, you can also set environment variables through the system settings.

---

## ğŸ” Data Validation, Transformation & Model Training

### âœ… Step 9: Data Validation
- Define schema in `config.schema.yaml`.
- Implement data validation functions in `utils.main_utils.py`.

### ğŸ”„ Step 10: Data Transformation
- Implement data transformation logic in `components.data_transformation.py`.
- Create `estimator.py` in the `entity` folder.

### ğŸ¤– Step 11: Model Training
- Define and implement model training steps in `components.model_trainer.py` using code from `estimator.py`.

---

## ğŸŒ AWS Setup for Model Evaluation & Deployment

### â˜ï¸ Step 12: AWS Setup
- Log in to the AWS console, create an IAM user, and grant `AdministratorAccess`.
- Set AWS credentials as environment variables:
```bash
# For Bash
export AWS_ACCESS_KEY_ID="YOUR_AWS_ACCESS_KEY_ID"
export AWS_SECRET_ACCESS_KEY="YOUR_AWS_SECRET_ACCESS_KEY"
```
- Configure an S3 bucket and add access keys in `constants.__init__.py`.

### ğŸ›¡ï¸ Step 13: Model Evaluation and Pushing to S3
- Create an S3 bucket named `my-model-mlopsproj` in the `us-east-1` region.
- Develop code to push/pull models to/from the S3 bucket in `src.aws_storage` and `entity/s3_estimator.py`.

---

## ğŸš€ Model Evaluation, Model Pusher, and Prediction Pipeline

### ğŸ§ª Step 14: Model Evaluation & Model Pusher
- Implement model evaluation and deployment components.
- Create the Prediction Pipeline and set up `app.py` for API integration.

### ğŸ–¥ï¸ Step 15: Static and Template Directory
- Add static and template directories for the web UI.

---

## ğŸ”„ CI/CD Setup with Docker, GitHub Actions, and AWS

### ğŸ³ Step 16: Docker and GitHub Actions
- Create `Dockerfile` and `.dockerignore`.
- Set up GitHub Actions with AWS authentication by creating secrets in GitHub for:
  - `AWS_ACCESS_KEY_ID`
  - `AWS_SECRET_ACCESS_KEY`
  - `AWS_DEFAULT_REGION`
  - `ECR_REPO`

### ğŸ–¥ï¸ Step 17: AWS EC2 and ECR
- Set up an EC2 instance for deployment.
- Install Docker on the EC2 machine.
- Connect EC2 as a self-hosted runner on GitHub.

### ğŸŒ Step 18: Final Steps
- Open the `5080` port on the EC2 instance.
- Access the deployed app by visiting:
  ```
  http://<public_ip>:5080
  ```

---

## ğŸ› ï¸ Additional Resources
- **Crash Course on setup.py and pyproject.toml**: See `crashcourse.txt` for details.
- **GitHub Secrets**: Manage secrets for secure CI/CD pipelines.

---

## ğŸ¯ Project Workflow Summary

- **Data Ingestion** â” **Data Validation** â” **Data Transformation**  
- **Model Training** â” **Model Evaluation** â” **Model Deployment**  
- **CI/CD Automation with GitHub Actions, Docker, AWS EC2, and ECR**

---

## ğŸ’¬ Connect
If you found this project helpful or have any questions, feel free to reach out!